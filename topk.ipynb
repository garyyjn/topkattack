{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc612d6-72a2-4fe0-a672-28b525f352c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ca7221-a458-447d-a94d-58cf1f5a9e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ext_yao_gary_mayo_edu/CLAM\n"
     ]
    }
   ],
   "source": [
    "#load reference csv\n",
    "reference_csv_path = '/home/jupyter/TCGA_lung.csv'\n",
    "TCGA_Lung_csv = pd.read_csv(reference_csv_path)\n",
    "\n",
    "#set feature(.h5 files path)\n",
    "features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/'\n",
    "list_feature_files = os.listdir(features_dir)\n",
    "\n",
    "#set clam path\n",
    "clam_dir = '/home/ext_yao_gary_mayo_edu/CLAM'\n",
    "%cd '/home/ext_yao_gary_mayo_edu/CLAM'\n",
    "\n",
    "#set model_loading_path\n",
    "model_checkpt_dir = './target_checkpoint.pt'\n",
    "\n",
    "#set train/val/test split path\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac53bd37-51e5-4faa-b4ae-49e3cb821217",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21305/3672744593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLAM_MB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mckpt_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'"
     ]
    }
   ],
   "source": [
    "from models.model_clam import CLAM_MB\n",
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "from torch.autograd import grad\n",
    "import openslide\n",
    "from datasets.dataset_h5 import eval_transforms\n",
    "from models.resnet_custom import resnet50_baseline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "e_transform = eval_transforms(pretrained = True)\n",
    "model = CLAM_MB(n_classes=2, dropout=True)\n",
    "ckpt_path = model_checkpt_dir\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt_clean = {}\n",
    "for key in ckpt.keys():\n",
    "    if 'instance_loss_fn' in key:\n",
    "        continue\n",
    "    ckpt_clean.update({key.replace('.module', ''):ckpt[key]})\n",
    "model.load_state_dict(ckpt_clean, strict=True)\n",
    "\n",
    "feature_extractor = resnet50_baseline(pretrained=True)\n",
    "feature_extractor.eval()\n",
    "model_defense = model_defense.cuda()\n",
    "feature_extractor = feature_extractor.cuda()\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "toTensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean = mean, std = std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7853a4c1-bc08-4502-9b40-9ee19cb8335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_methods = [method_name for method_name in dir(model_defense)\n",
    "                  if callable(getattr(model_defense, method_name))]\n",
    "print(object_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ddec7-08f6-46fe-afbb-d754fdf04d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pd.read_csv('/home/jupyter/splits.csv')\n",
    "train_file_list = splits['train'].values.tolist()\n",
    "test_file_list = splits['test'].values.tolist()\n",
    "val_file_list = splits['val'].values.tolist()\n",
    "train_file_list = [x for x in train_file_list if x == x]\n",
    "test_file_list = [x for x in test_file_list if x == x]\n",
    "val_file_list = [x for x in val_file_list if x == x]\n",
    "val_test_file_list = val_file_list + test_file_list\n",
    "full_list = val_test_file_list + train_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab080791-d6bc-40b3-931e-ced52d520ba9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Layer -1: Take forward_run output and produce accu and roctrain_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc05f6-6cb0-439b-8185-5daffc62da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(output, reference, answer_rows = ['c1', 'c2']):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for index, row in output.iterrows():\n",
    "        curr_slide_name = row['slide_name']\n",
    "        curr_c1 = float(row[answer_rows[0]])\n",
    "        curr_c2 = float(row[answer_rows[1]])\n",
    "        reference_row = reference[reference['slide_name'] == curr_slide_name]\n",
    "        reference_label = reference_row['label'].item()\n",
    "        if reference_label == 'LUAD':\n",
    "            if curr_c1 > curr_c2:\n",
    "                correct += 1\n",
    "        elif curr_c2 > curr_c1:\n",
    "            correct +=1 \n",
    "        total += 1\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d6b12-3e58-4ad5-8eb2-b0572f98a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(output, reference, answer_rows = ['c1', 'c2']):\n",
    "    #output is softmaxed\n",
    "    pred = output[answer_rows[0]].astype(np.float)\n",
    "    labels = []\n",
    "    for index, row in output.iterrows():\n",
    "        curr_slide_name = row['slide_name']\n",
    "        reference_row = reference[reference['slide_name'] == curr_slide_name]\n",
    "        reference_label = reference_row['label'].item()\n",
    "        if reference_label == 'LUAD':\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    y = np.array(labels)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f728c87-13ad-410d-b62e-4c7efd9d7b5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Layer -2: dataset level forward run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416cf0b-164e-4735-9bbf-51a4f685ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_dataset(model, features_dir, target_list, softmax = True, verbose = False, save = False, limit = -1):\n",
    "    list_feature_files = os.listdir(features_dir)\n",
    "    output = pd.DataFrame(columns = ['slide_name', 'c1', 'c2'])\n",
    "    for slide_name in tqdm(target_list):\n",
    "        full_feature_path = os.path.join(features_dir, '{}.h5'.format(slide_name))\n",
    "        try:\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "        except:\n",
    "            print('Failed on {}'.format(slide_name))\n",
    "            continue\n",
    "        features = torch.from_numpy(features).cuda()\n",
    "        l, y, y1, a, r = forward_ins(model, features, coords)\n",
    "        \n",
    "        if softmax:\n",
    "            c1 = y[0,0].detach().cpu().item()\n",
    "            c2 = y[0,1].detach().cpu().item()\n",
    "        else:\n",
    "            c1 = l[0,0].detach().cpu().item()\n",
    "            c2 = l[0,1].detach().cpu().item()\n",
    "        \n",
    "        output = output.append(pd.DataFrame([[slide_name, c1, c2]], columns = output.columns))\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49ad3e-06c0-490f-9402-359a1a8f0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_topk_attack(model, feature_extractor, reference, target_list, features_dir, slides_dir, results_save_dir = None, dataset_save_dir = None, topk = 10, e = 0.001, f = 0.001, limit = -1, save_dataset = False, attention_comp = False, verbose = False):\n",
    "    output = pd.DataFrame(columns = ['slide_name', 'c1_o', 'c2_o', 'c1_a', 'c2_a', 'c1_att', 'c2_att'])\n",
    "    feature_files = os.listdir(features_dir)\n",
    "    for slide_name in tqdm(target_list):\n",
    "        full_feature_path = os.path.join(features_dir, '{}.h5'.format(slide_name))\n",
    "        try:\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "            features = torch.from_numpy(features)\n",
    "            slide_full_path = os.path.join(slides_dir,\n",
    "                                           '{}.svs'.format(slide_name))\n",
    "            wsi = openslide.open_slide(slide_full_path)\n",
    "\n",
    "            reference_row = reference[reference['slide_name'] == slide_name]\n",
    "            reference_label = reference_row['label'].item()\n",
    "\n",
    "            slide_resolution = int(reference_row['resolution'])\n",
    "        except:\n",
    "            print('failed on {}'.format(slide_name))\n",
    "            continue\n",
    "        \n",
    "        if reference_label == 'LUAD':\n",
    "            attack_target = 0\n",
    "        else:\n",
    "            attack_target = 1\n",
    "        \n",
    "        if slide_resolution == 40:\n",
    "            patch_size = 512\n",
    "            downsample = 2\n",
    "        else:\n",
    "            patch_size = 256\n",
    "            downsample = 1\n",
    "        \n",
    "        y_collection, a_collection, idx_coord_list, img_collection, features_collection, noise_collection = instance_topk_attack(model, feature_extractor,\n",
    "                                                                                                               features, coords, wsi, patch_size = patch_size, downsample = downsample, \n",
    "                                                            target = attack_target, topk = topk, e = e , f = f, attention_comp = attention_comp)\n",
    "        y_o, y_a = y_collection[0:2]\n",
    "        a_o, a_a = a_collection[0:2]\n",
    "        img_a = img_collection[0]\n",
    "        features_a = features_collection[0]\n",
    "        if attention_comp:\n",
    "            y_att = y_collection[2]\n",
    "            a_att = a_collection[2]\n",
    "            img_att = img_collection[1]\n",
    "            features_att = features_collection[1]\n",
    "        \n",
    "        y_o = y_o.detach().cpu()\n",
    "        y_a = y_a.detach().cpu()\n",
    "        c1_o = y_o[0,0].item()\n",
    "        c2_o = y_o[0,1].item()\n",
    "        c1_a = y_a[0,0].item()\n",
    "        c2_a = y_a[0,1].item()\n",
    "        c1_att = 0\n",
    "        c2_att = 0\n",
    "    \n",
    "        if attention_comp:\n",
    "            c1_att = y_att[0,0].item()\n",
    "            c2_att = y_att[0,1].item()\n",
    "            \n",
    "        output = output.append(pd.DataFrame([[slide_name, c1_o, c2_o, c1_a, c2_a, c1_att, c2_att]], columns = output.columns))\n",
    "        \n",
    "        if save_dataset:\n",
    "            #dataset_save_dir has three folders: img, features and coords\n",
    "            #results_save_dir  has folders: att\n",
    "            img_save_path = os.path.join(dataset_save_dir, 'img')\n",
    "            feature_save_path = os.path.join(dataset_save_dir, 'features')\n",
    "            coords_save_path = os.path.join(dataset_save_dir, 'coords')\n",
    "            \n",
    "            attention_save_path = os.path.join(results_save_dir, 'att')\n",
    "            \n",
    "            \n",
    "            torch.save(img_a, os.path.join(img_save_path, '{}_adv.tr'.format(slide_name)))\n",
    "            torch.save(features_a, os.path.join(feature_save_path, '{}_adv.tr'.format(slide_name)))\n",
    "            with open(os.path.join(coords_save_path,\n",
    "                '{}_adv.p'.format(slide_name)), 'wb') as handle:\n",
    "                pickle.dump(idx_coord_list, handle)\n",
    "            \n",
    "            #todo attention\n",
    "            \n",
    "            if attention_comp:\n",
    "                torch.save(img_att, os.path.join(img_save_path, '{}_att.tr'.format(slide_name)))\n",
    "                torch.save(features_att, os.path.join(feature_save_path, '{}_att.tr'.format(slide_name)))\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0897cbbb-744a-4b52-b015-1ebed519fbb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Layer -3: instance level forward_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb87fe-d73e-4faa-86da-454ae99448d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_ins(model, features, coords):\n",
    "    model.cuda()\n",
    "    l, y, y1, a, r = model(features)\n",
    "    return l, y, y1, a, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf4ea0-4ed0-4f11-93b3-efee3c60e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_topk_attack(model, feature_extractor, features, coords, wsi,\n",
    "        patch_size = 256, patch_level = 0, downsample = 1, target = 1, topk = 10, e = .001,\n",
    "        f = 0.001, attention_comp = False, verbose = False):\n",
    "    \n",
    "    model = model.cuda()\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    \n",
    "    features = torch.clone(features).cuda()\n",
    "    features.requires_grad = True\n",
    "    \n",
    "    l, y, y1, a, r = model(features)\n",
    "    res, ind = a[target,:].topk(topk)\n",
    "    topk_coord = coords[ind.cpu(),:]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"model prediciton {}\".format(y))\n",
    "        print(\"top k attented values {}\".format(res))\n",
    "        print(\"top k attention index {}\".format(ind))\n",
    "        print(ind.shape)\n",
    "        print(coords.shape)\n",
    "        print(\"top k attented coord {}\".format(topk_coord))\n",
    "    if topk == 1:\n",
    "        topk_coord = [topk_coord]\n",
    "\n",
    "    features_copy = torch.clone(features)\n",
    "    \n",
    "    img_copy = torch.ones((topk,3,256,256)).cuda()\n",
    "    img_copy_unnormalized = torch.ones((topk,3,256,256)).cuda()\n",
    "    \n",
    "    attack_img_unnormalized = torch.ones((topk,3,256,256)).cuda()\n",
    "    attack_img_normalized = torch.ones((topk,3,256,256)).cuda()\n",
    "    \n",
    "    att_img_unnormalized = torch.ones((topk,3,256,256)).cuda()\n",
    "    att_img_normalized = torch.ones((topk,3,256,256)).cuda()\n",
    "    \n",
    "    idx_coord_list = []\n",
    "    \n",
    "    for i in range(topk):\n",
    "        coord_i = topk_coord[i]\n",
    "        idx_coord_list.append((ind[i],coord_i))\n",
    "        img = wsi.read_region(coord_i, patch_level,\n",
    "                (patch_size, patch_size)).convert('RGB')\n",
    "        if downsample != 1:\n",
    "            final_size = (int(patch_size/downsample), ) *2\n",
    "            img = img.resize(final_size)\n",
    "        \n",
    "        img = toTensor(img).cuda()\n",
    "        img_copy_unnormalized[i] = img\n",
    "        \n",
    "        img = normalize(img).unsqueeze(0)\n",
    "        img_copy[i] = img\n",
    "    \n",
    "    #topk attack\n",
    "    img_copy.requires_grad = True\n",
    "    img_copy_unnormalized.requires_grad = True\n",
    "    \n",
    "    new_features = feature_extractor(img_copy)\n",
    "    for i in range(topk):\n",
    "        features_copy[ind[i], :] = new_features[i]\n",
    "    l_original, y_original, y1_original, a_original, r = model(features_copy)\n",
    "    \n",
    "    loss = F.nll_loss(l_original, torch.tensor([target]).cuda())\n",
    "    loss.backward()\n",
    "    d_x_y = img_copy.grad.data\n",
    "    \n",
    "    attack_img_unnormalized = torch.clone(img_copy_unnormalized + d_x_y.sign() * e)\n",
    "    for i in range(topk):\n",
    "        attack_img_normalized[i] = normalize(attack_img_unnormalized[i])\n",
    "    attack_features = feature_extractor(attack_img_normalized)\n",
    "    \n",
    "    for i in range(topk):\n",
    "        features_copy[ind[i], :] = attack_features[i]\n",
    "    l_attacked, y_attacked, y1_attacked, a_attacked,r = model(features_copy)\n",
    "    \n",
    "    noise_collection = [d_x_y.sign().detach()]\n",
    "    l_collection = [l_original.detach(), l_attacked.detach()]\n",
    "    a_collection = [a_original.detach(), a_attacked.detach()]\n",
    "    y_collection = [y_original.detach(), y_attacked.detach()]\n",
    "    img_collection = [img_copy_unnormalized.detach(), attack_img_unnormalized.detach()]\n",
    "    features_collection= [attack_features.detach()]\n",
    "    \n",
    "    if attention_comp:\n",
    "        model.zero_grad()\n",
    "        feature_extractor.zero_grad()\n",
    "        attention_loss = torch.sum((a_attacked - a_original)**2)\n",
    "        \n",
    "        d_x_adiff = grad(attention_loss, attack_img_normalized)[0]\n",
    "        \n",
    "        att_img_unnormalized = attack_img_unnormalized - f * d_x_adiff.sign()\n",
    "        \n",
    "        for i in range(topk):\n",
    "            att_img_normalized[i] = normalize(att_img_unnormalized[i])\n",
    "        \n",
    "        attack_features_att = feature_extractor(att_img_normalized)\n",
    "\n",
    "        for i in range(topk):\n",
    "            features_copy[ind[i], :] = attack_features_att[i]\n",
    "\n",
    "        l_att, y_att, y1_att, a_att, r = model(features_copy)\n",
    "        new_attention_loss = torch.sum((a_att - a)**2)\n",
    "        #print('attenion loss {} new attention loss {}'.format(attention_loss, new_attention_loss))\n",
    "        l_collection.append(l_att.detach())\n",
    "        a_collection.append(a_att.detach())\n",
    "        y_collection.append(y_att.detach())\n",
    "        noise_collection.append(d_x_adiff.detach())\n",
    "        img_collection.append(att_img_unnormalized.detach())\n",
    "        features_collection.append(attack_features_att.detach())\n",
    "        \n",
    "    return y_collection, a_collection, idx_coord_list, img_collection, features_collection, noise_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8d9b8-000d-455c-9143-e2a7722df734",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# dropout no dropout experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd6ad2-6f03-414c-8229-2af25df7fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = CLAM_MB(n_classes=2, dropout=True)\n",
    "ckpt_path = '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt_clean = {}\n",
    "for key in ckpt.keys():\n",
    "    if 'instance_loss_fn' in key:\n",
    "        continue\n",
    "    ckpt_clean.update({key.replace('.module', ''):ckpt[key]})\n",
    "model_dropout.load_state_dict(ckpt_clean, strict=True)\n",
    "\n",
    "model_no_dropout = CLAM_MB(n_classes=2, dropout=False)\n",
    "\n",
    "ckpt_path = '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt_clean = {}\n",
    "for key in ckpt.keys():\n",
    "    if 'instance_loss_fn' in key:\n",
    "        continue\n",
    "    key_new = key.replace('attention_net.module.3.attention', 'attention_net.module.2.attention')\n",
    "    ckpt_clean.update({key_new.replace('.module', ''):ckpt[key]})\n",
    "    \n",
    "model_no_dropout.load_state_dict(ckpt_clean, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b10c1-614e-4539-9ca1-6ddc2a820e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_output = forward_dataset(model_dropout, features_dir='/home/ext_yao_gary_mayo_edu/lung-features/')\n",
    "no_dropout_output = forward_dataset(model_no_dropout, features_dir='/home/ext_yao_gary_mayo_edu/lung-features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec6b83-692f-4f72-922c-c729c4b642b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d0424-04fc-4cf2-8991-3eea61dfbf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_accu, dropout_auc = accu(dropout_output, answer_rows = ['c1','c2'], reference = TCGA_Lung_csv), auc(dropout_output, answer_rows = ['c1','c2'], reference = TCGA_Lung_csv)\n",
    "no_dropout_accu, no_dropout_auc = accu(no_dropout_output, answer_rows = ['c1', 'c2'], reference = TCGA_Lung_csv), auc(no_dropout_output, answer_rows = ['c1', 'c2'], reference = TCGA_Lung_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b80548-fc12-4e9a-ae3a-42979160960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dropout_accu, dropout_auc)\n",
    "print(no_dropout_accu, no_dropout_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72520a-4b2c-4933-80a5-13df686b5f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eef9da-06e5-471e-a43d-a965bc7a1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = forward_dataset(model_defense, '/home/ext_yao_gary_mayo_edu/lung-features/h5_files/', val_test_file_list)\n",
    "print(accu(output, TCGA_Lung_csv), auc(output, TCGA_Lung_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2d8bc-ccab-46b7-b67b-eabe3fa35ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dataset_topk_attack(model_defense, feature_extractor, TCGA_Lung_csv, val_test_file_list,\n",
    "                             '/home/ext_yao_gary_mayo_edu/lung-features/h5_files/', '/home/ext_yao_gary_mayo_edu/TCGA-Lung-Slides',\n",
    "                             e = 0.008, f = 0.008, limit = 100, attention_comp = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dd63a-3d2c-470a-854b-14e981036999",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy, baseline_auc = accu(output, TCGA_Lung_csv, \n",
    "                                       answer_rows = ['c1_o', 'c2_o']), auc(output, TCGA_Lung_csv, answer_rows = ['c1_o', 'c2_o'])\n",
    "print(baseline_accuracy, baseline_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cc61f-a9ef-4bec-a79e-1156299ae17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked_accuracy, attacked_auc = accu(output, TCGA_Lung_csv,\n",
    "                                       answer_rows = ['c1_a', 'c2_a']),auc(output, TCGA_Lung_csv, answer_rows = ['c1_a', 'c2_a'])\n",
    "print(attacked_accuracy, attacked_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e40d87-0cb2-4ff8-9a0c-6846e3af933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attented_accuracy, attented_auc = accu(output, TCGA_Lung_csv, \n",
    "                                       answer_rows = ['c1_att', 'c2_att']), auc(output, TCGA_Lung_csv, answer_rows = ['c1_att', 'c2_att'])\n",
    "print(attented_accuracy, attented_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d0afc-6eb8-483c-bbd4-bf0a7d341091",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_results_path = '/home/jupyter/topk_datasets/validation_saves_final'\n",
    "default_datasets_path = '/home/jupyter/topk_datasets/validation_datasets_final'\n",
    "curr_results_path = os.path.join(default_results_path, 'ee2fe2')\n",
    "curr_datasets_path = os.path.join(default_datasets_path, 'ee2fe2')\n",
    "def gen_save_folders(dataset_path, results_path):\n",
    "    if os.path.isdir(dataset_path):\n",
    "        shutil.rmtree(dataset_path)\n",
    "    if os.path.isdir(results_path):\n",
    "        shutil.rmtree(results_path)\n",
    "    os.mkdir(dataset_path)\n",
    "    os.mkdir(results_path)\n",
    "    os.mkdir(os.path.join(dataset_path, 'img'))\n",
    "    os.mkdir(os.path.join(dataset_path, 'features'))\n",
    "    os.mkdir(os.path.join(dataset_path, 'coords'))\n",
    "\n",
    "    os.mkdir(os.path.join(results_path, 'att'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e18fed-4b24-4c8c-ab38-94278446794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = [0.001, 0.005, 0.01, 0.03, 0.06, 0.1]\n",
    "save_name_list = ['e3e', 'e5e3', 'e2e', 'e32e', 'e62e', 'e1e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235eb66c-4e76-40f0-850b-b635da21fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, save_name in zip(e_list, save_name_list):\n",
    "    curr_results_path = os.path.join(default_results_path, save_name)\n",
    "    curr_datasets_path = os.path.join(default_datasets_path, save_name)\n",
    "    gen_save_folders(dataset_path=curr_datasets_path, results_path = curr_results_path)\n",
    "    output = dataset_topk_attack(model_defense, feature_extractor, TCGA_Lung_csv, val_test_file_list,\n",
    "                             '/home/ext_yao_gary_mayo_edu/lung-features/h5_files/', '/home/ext_yao_gary_mayo_edu/TCGA-Lung-Slides',\n",
    "                             results_save_dir=curr_results_path, dataset_save_dir=curr_datasets_path, save_dataset = True, e = e, f = e, topk = 10, limit = -1, attention_comp = True)\n",
    "    output.to_csv('/home/jupyter/spreads_val/ts{}.csv'.format(e),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65917544-1064-4616-8227-52a902fa95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [1,3,5,10,15,20,30,40,50]\n",
    "save_name_list = ['k1', 'k3', 'k5', 'k10', 'k15', 'k20', 'k30', 'k40']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac622f2-dcd6-410b-bb6d-199082aa2c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, save_name in zip(k_list, save_name_list):\n",
    "    curr_results_path = os.path.join(default_results_path, save_name)\n",
    "    curr_datasets_path = os.path.join(default_datasets_path, save_name)\n",
    "    gen_save_folders(dataset_path=curr_datasets_path, results_path = curr_results_path)\n",
    "    output = dataset_topk_attack(model_defense, feature_extractor, TCGA_Lung_csv, val_test_file_list,\n",
    "                             '/home/ext_yao_gary_mayo_edu/lung-features/h5_files/', '/home/ext_yao_gary_mayo_edu/TCGA-Lung-Slides',\n",
    "                             results_save_dir=curr_results_path, dataset_save_dir=curr_datasets_path, save_dataset = True, e = 0.01, f = 0.01, topk = k, limit = -1, attention_comp = True)\n",
    "    output.to_csv('/home/jupyter/spreads/new{}.csv'.format(save_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85282763-8da8-4a0d-9052-d6795013d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = [.01]\n",
    "save_name_list = ['e1e_whole_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a51e32-e902-4743-a99f-76a473b66efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, save_name in zip(e_list, save_name_list):\n",
    "    curr_results_path = os.path.join(default_results_path, save_name)\n",
    "    curr_datasets_path = os.path.join(default_datasets_path, save_name)\n",
    "    gen_save_folders(dataset_path=curr_datasets_path, results_path = curr_results_path)\n",
    "    output = dataset_topk_attack(model_defense, feature_extractor, TCGA_Lung_csv, val_test_file_list,\n",
    "                             '/home/ext_yao_gary_mayo_edu/lung-features/h5_files/', '/home/ext_yao_gary_mayo_edu/TCGA-Lung-Slides',\n",
    "                             results_save_dir=curr_results_path, dataset_save_dir=curr_datasets_path, save_dataset = True, e = e, f = e/2, topklimit = -1, attention_comp = True)\n",
    "    output.to_csv('/home/jupyter/spreads_val/{}.csv'.format(save_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5e511-19a6-4a28-aba6-daf38789f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/jupyter/spreads_val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4ba75-5c4d-4e87-aa42-38d4f4ed7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_save = 'e1e'\n",
    "full_csv_path = os.path.join(output_path, '{}.csv'.format(curr_save))\n",
    "curr_output = pd.read_csv(full_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e78bd-b03b-4bba-99e6-f473fbbd3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compilation_csv = pd.DataFrame(columns = ['save_name', 'baseline_accu', 'baseline_auc', 'attacked_accu', 'attacked_auc', 'att_accu', 'att_auc'])\n",
    "for file_name in os.listdir(output_path):\n",
    "    full_csv_path = os.path.join(output_path, file_name)\n",
    "    try:\n",
    "        curr_output = pd.read_csv(full_csv_path)\n",
    "    except:\n",
    "        continue\n",
    "    save_name = file_name[:-4]\n",
    "    baseline_accuracy, baseline_auc = accu(curr_output, TCGA_Lung_csv, answer_rows = ['c1_o', 'c2_o']), auc(curr_output, TCGA_Lung_csv, answer_rows = ['c1_o', 'c2_o'])\n",
    "    attacked_accuracy, attacked_auc = accu(curr_output, TCGA_Lung_csv, answer_rows = ['c1_a', 'c2_a']), auc(curr_output, TCGA_Lung_csv, answer_rows = ['c1_a', 'c2_a'])\n",
    "    att_accuracy, att_auc = accu(curr_output, TCGA_Lung_csv, answer_rows = ['c1_att', 'c2_att']), auc(curr_output, TCGA_Lung_csv, answer_rows = ['c1_att', 'c2_att'])\n",
    "    compilation_csv = compilation_csv.append(pd.DataFrame([['{}'.format(save_name), baseline_accuracy, baseline_auc, attacked_accuracy, attacked_auc, att_accuracy, att_auc]], columns = compilation_csv.columns))\n",
    "compilation_csv.to_csv('/home/jupyter/val_compliation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab526e90-15fb-4acc-bd2a-c6842e30f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /home/jupyter/spreads/.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859eff1-d4d7-4028-9b28-28ac61b0f681",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Attention Percentage experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785262a-14f4-42ab-8573-ac681c5b9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_attention_ratio(a, threshholds):\n",
    "    #input a is raw attention for one class\n",
    "    output = []\n",
    "    a_softmax = torch.softmax(a, dim = 0)\n",
    "    for threshhold in threshholds:\n",
    "        res, ind = a_softmax.topk(threshhold)\n",
    "        output.append(torch.sum(res).item())\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4a623-b324-49eb-a2f8-bcf1131a622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_attention_analysis(model, feature_extractor, reference, slide_name, feature_dir, slide_dir, e = 0.01, f = 0.01, topk = 10):\n",
    "    full_feature_path = os.path.join(features_dir, '{}.h5'.format(slide_name))\n",
    "    full_slide_path = os.path.join(slide_dir, '{}.svs'.format(slide_name))\n",
    "    with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "        features = hdf5_file['features'][:]\n",
    "        coords = hdf5_file['coords'][:]\n",
    "        features = torch.from_numpy(features)\n",
    "        wsi = openslide.open_slide(full_slide_path)\n",
    "\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        reference_label = reference_row['label'].item()\n",
    "\n",
    "        slide_resolution = int(reference_row['resolution'])\n",
    "            \n",
    "    if reference_label == 'LUAD':\n",
    "        attack_target = 0\n",
    "    else:\n",
    "        attack_target = 1\n",
    "        \n",
    "    if slide_resolution == 40:\n",
    "        patch_size = 512\n",
    "        downsample = 2\n",
    "    else:\n",
    "        patch_size = 256\n",
    "        downsample = 1\n",
    "    \n",
    "    y_collection, a_collection, idx_coord_list, img_collection, features_collection, noise_collection = instance_topk_attack(model, feature_extractor, features, coords, wsi, patch_size = patch_size, downsample = downsample, \n",
    "                                                            target = attack_target, topk = topk, e = e , f = f, attention_comp = True)\n",
    "    y_o, y_a, y_att = y_collection\n",
    "    a_o, a_a, a_att = a_collection\n",
    "    img_a = img_collection[0]\n",
    "    features_a = features_collection[0]\n",
    "    \n",
    "    res, original_topk_ind = a_o.topk(topk)\n",
    "    \n",
    "    a_o = softmax(a_o.detach().cpu().numpy(), axis = 1)\n",
    "    a_a = softmax(a_a.detach().cpu().numpy(), axis = 1)\n",
    "    a_att = softmax(a_att.detach().cpu().numpy(), axis = 1)\n",
    "    original_topk_ind = original_topk_ind.detach().cpu().numpy()\n",
    "    '''\n",
    "    print(original_topk_ind)\n",
    "    print(a_o[:,original_topk_ind])\n",
    "    print(a_a[:,original_topk_ind])\n",
    "    print(a_att[:,original_topk_ind])\n",
    "    print(y_o, y_a, y_att)\n",
    "    '''\n",
    "    labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'remaining attention scores']\n",
    "    fig, ax = plt.subplots(3, 2)\n",
    "    set_size(20,20, ax[0,0])\n",
    "    sizes_nat_0 = a_o[0][original_topk_ind[0,:-1]].tolist()\n",
    "    sizes_nat_0.append(1 - sum(sizes_nat_0))\n",
    "    sizes_nat_1 = a_o[1][original_topk_ind[1,:-1]].tolist()\n",
    "    sizes_nat_1.append(1 - sum(sizes_nat_1))\n",
    "    print(sizes_nat_0)\n",
    "    ax[0,0].pie(sizes_nat_0, labels = labels)\n",
    "    ax[0,1].pie(sizes_nat_1, labels = labels)\n",
    "    ax[0,0].set_title('Softmax Attention Shares, Natural, class 1')\n",
    "    ax[0,1].set_title('Softmax Attention Shares, Natural, class 2')\n",
    "    sizes_adv_0 = a_a[0][original_topk_ind[0,:-1]].tolist()\n",
    "    sizes_adv_0.append(1 - sum(sizes_adv_0))\n",
    "    sizes_adv_1 = a_a[1][original_topk_ind[1,:-1]].tolist()\n",
    "    sizes_adv_1.append(1 - sum(sizes_adv_1))\n",
    "    print(sizes_adv_0)\n",
    "    ax[1,0].pie(sizes_adv_0, labels = labels)\n",
    "    ax[1,1].pie(sizes_adv_1, labels = labels)\n",
    "    ax[1,0].set_title('Softmax Attention Shares, Topk Attacked e = 0.01, class 1')\n",
    "    ax[1,1].set_title('Softmax Attention Shares, Topk Attacked e = 0.01, class 2')\n",
    "    sizes_att_0 = a_att[0][original_topk_ind[0,:-1]].tolist()\n",
    "    sizes_att_0.append(1 - sum(sizes_att_0))\n",
    "    sizes_att_1 = a_att[1][original_topk_ind[1,:-1]].tolist()\n",
    "    sizes_att_1.append(1 - sum(sizes_att_1))\n",
    "    print(sizes_att_0)\n",
    "    ax[2,0].pie(sizes_att_0, labels = labels)\n",
    "    ax[2,1].pie(sizes_att_1, labels = labels)\n",
    "    ax[2,0].set_title('Softmax Attention Shares, Topk with Compensastion e,f = 0.01, class 1')\n",
    "    ax[2,1].set_title('Softmax Attention Shares, Topk with Compensastion e,f = 0.01, class 2')\n",
    "    plt.suptitle('Attention Hijacking Demostration, slide_id = {}'.format(slide_name[:10]))\n",
    "    plt.show()\n",
    "    #print(a_o[0][original_topk_ind[0,:]].shape)\n",
    "    #pie_chart(a_o[0][original_topk_ind[0,:]].tolist())\n",
    "   # pie_chart(a_a[0][original_topk_ind[0,:]].tolist())\n",
    "    #pie_chart(a_att[0][original_topk_ind[0,:]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ee136-37f7-4950-902c-2b24ad733975",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_attention_analysis(model_defense, feature_extractor, reference = TCGA_Lung_csv, slide_name = 'TCGA-49-6744-01Z-00-DX2.1982e585-65a4-4330-9140-ccabcdd106f8', feature_dir = '/home/ext_yao_gary_mayo_edu/lung-features/', slide_dir = '/home/ext_yao_gary_mayo_edu/lung-slides/', e = .001, f = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69f051-789d-4833-92ab-bf40188a17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "def pie_chart(sizes):\n",
    "    # Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "    print(sizes)\n",
    "    sizes.append(1 - sum(sizes))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(sizes, autopct='%1.1f%%')\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.\n",
    "    ax.set_title('Natural topk attention softmax')\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34966f1f-6879-459b-aed1-8e7c11750e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Visualization Demostration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c6d28-33a6-4907-80c2-da1ae32e596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d7e27-9304-45d8-adef-73df9a1f17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/home/jupyter/airplane.jpeg')\n",
    "a = transforms.ToTensor()\n",
    "tensored_img = a(img)\n",
    "untensored_img = untransform(tensored_img)\n",
    "display(untensored_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b4b17-81c7-43cd-9311-686983c3ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untransform(img_array_transformed):\n",
    "    #3*w*h tensor back to img\n",
    "    #if torch.max(img_array_transformed) > 1:\n",
    "     #   img_array_transformed[img_array_transformed > 1] = 1\n",
    "    img_array = img_array_transformed.transpose(0,1).transpose(1,2)\n",
    "    img_array = img_array.numpy()\n",
    "    img = Image.fromarray((img_array*255).astype('uint8'), 'RGB')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e7a44-8244-47c7-a1cc-96d35f77603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4f588-80a4-4f56-8d52-7114826276d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_visualization_analytics(model, feature_extractor, reference, slide_name, features_dir, slides_dir, topk = 10, e = [0.01, 0.03, 0.05, 0.1, 0.3, 0.5], return_idx = None):\n",
    "    \n",
    "    full_feature_path = os.path.join(features_dir, '{}.h5'.format(slide_name))\n",
    "    with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "        features = hdf5_file['features'][:]\n",
    "        coords = hdf5_file['coords'][:]\n",
    "        features = torch.from_numpy(features)\n",
    "        slide_full_path = os.path.join(slides_dir,\n",
    "                                       '{}.svs'.format(slide_name))\n",
    "        wsi = openslide.open_slide(slide_full_path)\n",
    "\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        reference_label = reference_row['label'].item()\n",
    "\n",
    "        slide_resolution = int(reference_row['resolution'])\n",
    "        \n",
    "        if reference_label == 'LUAD':\n",
    "            attack_target = 0\n",
    "        else:\n",
    "            attack_target = 1\n",
    "        \n",
    "        if slide_resolution == 40:\n",
    "            patch_size = 512\n",
    "            downsample = 2\n",
    "        else:\n",
    "            patch_size = 256\n",
    "            downsample = 1\n",
    "    \n",
    "    y_collection, a_collection, idx_coord_list, img_collection, features_collection, noise_collection = instance_topk_attack(model, \n",
    "        feature_extractor, features, coords, wsi,\n",
    "        patch_size = patch_size, patch_level = 0, downsample = downsample, target = attack_target, topk = topk, e = 0.01,\n",
    "        f = 0.01, attention_comp = False, verbose = False)\n",
    "    \n",
    "    y_original, y_attack = y_collection\n",
    "    print('original y {} {}'.format(y_original[0,0].item(), y_original[0,1].item()))\n",
    "    e_variation_num = len(e)\n",
    "    plt.rcParams['axes.facecolor']='white'\n",
    "    plt.rcParams['savefig.facecolor']='white'\n",
    "    fig, ax = plt.subplots(2+e_variation_num, topk)\n",
    "    figure(figsize=(100, 100))\n",
    "    img_original, img_attack = img_collection\n",
    "    a_original, a_attack = a_collection\n",
    "    attack_noise = noise_collection[0]\n",
    "    topk_idx_list = []\n",
    "    if return_idx is not None:\n",
    "        return_i, return_j = return_idx\n",
    "    return_item = None\n",
    "    for i in range(topk):\n",
    "        idx, coord = idx_coord_list[i]\n",
    "        print(idx)\n",
    "        print(coord)\n",
    "        topk_idx_list.append(idx)\n",
    "        \n",
    "    for i, current_img_tensor in enumerate(img_original):\n",
    "        current_attention = a_original[:,topk_idx_list[i]]\n",
    "        temp_test_img = torch.unsqueeze(current_img_tensor, dim = 0)\n",
    "        temp_feats = feature_extractor(temp_test_img)\n",
    "        \n",
    "        #print(temp_feats.shape)\n",
    "        l,y,y1,a,r = model(temp_feats)\n",
    "        c0 = y[0,0].item()\n",
    "        c1 = y[0,1].item()\n",
    "        current_img = untransform(current_img_tensor.cpu())\n",
    "        ax[0, i].imshow(current_img)\n",
    "        ax[0, i].set_axis_off()\n",
    "        ax[0, i].set_title(\"{}, [{}, {}]\".format(str(round(current_attention[attack_target].item(),2)), \n",
    "                                                   str(round(c0,2)), str(round(c1,2))),\n",
    "                           color = 'white')\n",
    "        set_size(15,15, ax[0,i])\n",
    "    for i, current_noise_tensor in enumerate(attack_noise):\n",
    "        current_noise_img = untransform(current_noise_tensor.cpu())\n",
    "        ax[1, i].imshow(current_noise_img)\n",
    "        ax[1, i].set_axis_off()\n",
    "        \n",
    "    for j, current_e in enumerate(e):\n",
    "        y_collection, a_collection, idx_coord_list, img_collection, features_collection, noise_collection = instance_topk_attack(model, feature_extractor, features, coords, wsi,patch_size = patch_size, patch_level = 0, downsample = downsample, target = attack_target, topk = topk, e = current_e, f = 0.01, attention_comp = False, verbose = False)\n",
    "        a_original, a_attack = a_collection\n",
    "        y_original, y_attack = y_collection\n",
    "        print('attacked at e = {}, {} {}'.format(current_e, y_attack[0,0].item(), y_attack[0,1].item()))\n",
    "        for i, (current_img_tensor, current_noise_tensor) in enumerate(zip(img_original, attack_noise)):\n",
    "            current_attention = a_attack[:,topk_idx_list[i]]\n",
    "            current_attacked_img_tensor = current_img_tensor + current_e * current_noise_tensor\n",
    "            \n",
    "            temp_test_img = torch.unsqueeze(current_attacked_img_tensor, dim = 0)\n",
    "            temp_feats = feature_extractor(temp_test_img)\n",
    "            l,y,y1,a,r = model(temp_feats)\n",
    "            c0 = y[0,0].item()\n",
    "            c1 = y[0,1].item()\n",
    "            \n",
    "            current_attacked_img = untransform(current_attacked_img_tensor.cpu())\n",
    "            ax[j+2,i].imshow(current_attacked_img)\n",
    "            ax[j+2,i].set_axis_off()\n",
    "            ax[j+2,i].set_title(\"{}, [{}, {}]\".format(str(round(current_attention[attack_target].item(),2)), \n",
    "                                                   str(round(c0,2)), str(round(c1,2))),\n",
    "                           color = 'white')\n",
    "            if j+2 == return_j and i == return_i:\n",
    "                return_item = current_attacked_img\n",
    "    plt.show()\n",
    "    return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dc55b-cfe1-436e-92f6-0679c85bc8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/'\n",
    "slides_dir = '/home/ext_yao_gary_mayo_edu/lung-slides'\n",
    "target_slide = 'TCGA-49-6744-01Z-00-DX2.1982e585-65a4-4330-9140-ccabcdd106f8'\n",
    "moded_center = instance_visualization_analytics(model_defense, feature_extractor, TCGA_Lung_csv, target_slide, features_dir, slides_dir, return_idx = [4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee44ac9-56b2-4e35-bb29-e57566986578",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(moded_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c58384-3db7-48eb-975d-b4686dbaed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_amoung_many(slide_name, reference, target_coords, wsi, moded_center, e = 0.01, dist = 1):\n",
    "    reference_row = reference[reference['slide_name'] == slide_name]\n",
    "    reference_label = reference_row['label'].item()\n",
    "\n",
    "    slide_resolution = int(reference_row['resolution'])\n",
    "\n",
    "    if slide_resolution == 40:\n",
    "        patch_size = 512\n",
    "        downsample = 2\n",
    "    else:\n",
    "        patch_size = 256\n",
    "        downsample = 1\n",
    "    target_l, target_w = target_coords\n",
    "    dim_l, dim_w = patch_size*(1+(2*dist)), patch_size*(1+(2*dist))\n",
    "    start_l = target_l - dist*patch_size\n",
    "    start_w = target_w - dist*patch_size\n",
    "    print((start_l, start_w), (dim_l, dim_w))\n",
    "    img = wsi.read_region((start_l, start_w), 0,\n",
    "                (dim_l, dim_w)).convert('RGB')\n",
    "    if downsample != 1:\n",
    "        final_size = (int(dim_l/downsample), ) * 2\n",
    "        img = img.resize(final_size)\n",
    "    img_array = np.array(img)\n",
    "    moded_center_array = np.array(moded_center)\n",
    "    print(moded_center_array.shape)\n",
    "    print(img_array.shape)\n",
    "    img_array[dist*256:(dist+1)*256, dist*256:(dist+1)*256, :] = np.array(moded_center)\n",
    "    img = Image.fromarray(img_array, 'RGB')\n",
    "    \n",
    "    display(img)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1075ca0-7c3c-4b2d-8670-4ca97ce8f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/'\n",
    "slides_dir = '/home/ext_yao_gary_mayo_edu/lung-slides'\n",
    "target_slide = 'TCGA-49-6744-01Z-00-DX2.1982e585-65a4-4330-9140-ccabcdd106f8'\n",
    "\n",
    "slide_full_path = os.path.join(slides_dir,\n",
    "                               '{}.svs'.format(target_slide))\n",
    "wsi = openslide.open_slide(slide_full_path)\n",
    "generate_one_amoung_many(target_slide, TCGA_Lung_csv, (49312,33184), wsi, moded_center, dist = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8ab41-5ba8-4edd-a810-d5983c34ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pathology_exp(e_levels = [0.05, 0.1, 0.3], ):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ed67a-5ba1-44b1-b048-a2723f3c80d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Defense Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68796d58-6607-486c-817b-8cd4069588ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_accu(output, target_columns = ['natural_used', 'natural_total']):\n",
    "    total_used = 0\n",
    "    total_tiles = 0\n",
    "    for index, row in output.iterrows():\n",
    "        slide_used = int(row[target_columns[0]])\n",
    "        slide_total = int(row[target_columns[1]])\n",
    "        total_used += slide_used\n",
    "        total_tiles += slide_total\n",
    "    return total_used, total_tiles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4595577-b1a0-4959-9d35-3504ffcc0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defense_validation(model, feature_extractor, features_dir, adv_dataset_dir, reference, val_list, defense = 'std_filter', d = 2.5, full_pipe = False, attention_comp = False, softmax = True, verbose = False, save = False, limit = -1, natural = False):\n",
    "    #validated on e1e good!\n",
    "    #dataset_save_dir has three folders: img, features and coords\n",
    "    list_feature_files = os.listdir(features_dir)\n",
    "    output = pd.DataFrame(columns = ['slide_name', 'c1_undefended', 'c2_undefended', 'c1_defended', 'c2_defended', 'natural_used', 'natural_total', 'adv_used', 'adv_total'])\n",
    "    for slide_name in tqdm(val_list):\n",
    "        full_feature_path = os.path.join(features_dir, '{}.h5'.format(slide_name))\n",
    "        try:\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "        except:\n",
    "            pring('failre on {}'.format(slide_name))\n",
    "            continue\n",
    "        features_original = torch.from_numpy(features).cuda()\n",
    "        #load adv\n",
    "        if not attention_comp:\n",
    "            adv_feature_path = os.path.join(adv_dataset_dir, 'features', '{}_adv.tr'.format(slide_name))\n",
    "            adv_coords_path = os.path.join(adv_dataset_dir, 'coords', '{}_adv.p'.format(slide_name))\n",
    "            adv_img_path = os.path.join(adv_dataset_dir, 'img', '{}_adv.tr'.format(slide_name))\n",
    "        else:\n",
    "            adv_feature_path = os.path.join(adv_dataset_dir, 'features', '{}_att.tr'.format(slide_name))\n",
    "            adv_coords_path = os.path.join(adv_dataset_dir, 'coords', '{}_adv.p'.format(slide_name))\n",
    "            adv_img_path = os.path.join(adv_dataset_dir, 'img', '{}_att.tr'.format(slide_name))\n",
    "        try:\n",
    "            idx_coords = pickle.load(open(adv_coords_path, 'rb'))\n",
    "            adv_features = torch.load(adv_feature_path)\n",
    "            adv_img = torch.load(adv_img_path)\n",
    "            if full_pipe:\n",
    "                adv_features = feature_extractor(adv_img)\n",
    "        except:\n",
    "            print('Failed on {}'.format(slide_name))\n",
    "            continue\n",
    "        #hacki implemeantation\n",
    "        k = len(idx_coords)\n",
    "        l, y, y1, a, r = forward_ins(model, features_original, coords)\n",
    "        total = features_original.shape[0]\n",
    "        adv_total = 10\n",
    "        natural_total = total - adv_total\n",
    "        adv_used = 0\n",
    "        natural_used = 0\n",
    "        c1 = y[0,0].detach().cpu().item()\n",
    "        c2 = y[0,1].detach().cpu().item()\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        reference_label = reference_row['label'].item()\n",
    "        if reference_label == 'LUAD':\n",
    "            target = 0\n",
    "        else:\n",
    "            target = 1\n",
    "        \n",
    "        res, ind = a[target,0:].topk(k)\n",
    "        if not natural:\n",
    "            for i in range(k):\n",
    "                features_original[ind[i], :] = adv_features[i]\n",
    "    \n",
    "        if defense == 'std_filter' or defense == 'topk_filter':\n",
    "            l_defended, y_defended, y1, a, r = model.forward_with_defense(features_original, defense, d)\n",
    "            mask = model.get_defense_mask(features_original, defense, d)\n",
    "            total_used = torch.sum(mask.int())\n",
    "            adv_mask = mask[ind]\n",
    "            adv_used = torch.sum(adv_mask.int()).item()\n",
    "            natural_used = total_used - adv_used\n",
    "        l_undefended, y_undefended, y1, a, r = model(features_original)\n",
    "        \n",
    "        #l, y, y1, a, r = forward_ins(model, features_original, coords)\n",
    "        if softmax:\n",
    "            c1_defended = y_defended[0,0].detach().cpu().item()\n",
    "            c2_defended = y_defended[0,1].detach().cpu().item()\n",
    "        else:\n",
    "            c1_adv = l[0,0].detach().cpu().item()\n",
    "            c2_adv = l[0,1].detach().cpu().item()\n",
    "        \n",
    "        c1_undefended = y_undefended[0,0].detach().cpu().item()\n",
    "        c2_undefended = y_undefended[0,1].detach().cpu().item()\n",
    "        output = output.append(pd.DataFrame([[slide_name, c1_undefended, c2_undefended, c1_defended, c2_defended, natural_used, natural_total, adv_used, adv_total]], columns = output.columns))\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689818b9-e500-4075-b9ac-ea313563ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saves_dir = '/home/jupyter/topk_datasets/datasets/'\n",
    "saves = os.listdir('/home/jupyter/topk_datasets/datasets/')\n",
    "saves.remove('.ipynb_checkpoints')\n",
    "compliation_csv = pd.DataFrame(columns = ['save_name', 'accu_undefended', 'auc_undefended', 'accu_defended', 'auc_defended',' natural_tiles_used%', 'adv_tiles_used%'])\n",
    "#compliation_csv = pd.DataFrame(columns = ['save_name', 'accu_undefended', 'auc_undefended', 'accu_defended', 'auc_defended'])\n",
    "print(saves)\n",
    "att_comp = False\n",
    "#d_list = [1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0]\n",
    "#d_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "d_list = [1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.55, 2.6, 2.65, 2.7, 2.75, 2.8, 2.85, 2.9, 2.95, 3.0, 3.05, 3.1, 3.15, 3.2, 3.25, 3.3, 3.35, 3.4, 3.45, 3.5, 3.55, 3.6, 3.65, 3.7, 3.75, 3.8, 3.85, 3.9, 3.95, 4.0]\n",
    "for d_curr in d_list:\n",
    "    output = defense_validation(model_defense, feature_extractor, features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_dir = '/home/jupyter/topk_datasets/datasets/{}'.format('e2e'), reference = TCGA_Lung_csv, val_list = val_test_file_list\n",
    "                                   ,defense = 'std_filter', d = d_curr, attention_comp = att_comp)\n",
    "    accu_und, auc_und  = accu(output, answer_rows=['c1_undefended', 'c2_undefended'], reference = TCGA_Lung_csv), auc(output, answer_rows=['c1_undefended', 'c2_undefended'], reference = TCGA_Lung_csv) \n",
    "    accu_def, auc_def = accu(output, answer_rows=['c1_defended', 'c2_defended'], reference = TCGA_Lung_csv), auc(output, answer_rows=['c1_defended', 'c2_defended'], reference = TCGA_Lung_csv)\n",
    "    \n",
    "    natural_used, natural_total = tile_accu(output, target_columns = ['natural_used', 'natural_total'])\n",
    "    adv_used, adv_total = tile_accu(output, target_columns = ['adv_used', 'adv_total'])\n",
    "    #print(natural_used, natural_total, adv_used, adv_total)\n",
    "    compliation_csv = compliation_csv.append(pd.DataFrame([['{}'.format(d_curr), accu_und, auc_und, accu_def, auc_def, (natural_used/natural_total), (adv_used/adv_total)]], columns = compliation_csv.columns))\n",
    "    #compliation_csv = compliation_csv.append(pd.DataFrame([['{}'.format(d_curr), accu_und, auc_und, accu_def, auc_def]], columns = compliation_csv.columns))\n",
    "    compliation_csv.to_csv('/home/jupyter/defensive_complicatione2e_noatcomp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920d61e-f60b-48e0-9d53-4df9a52336fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "saves_dir = '/home/jupyter/topk_datasets/datasets/'\n",
    "saves = os.listdir('/home/jupyter/topk_datasets/datasets/')\n",
    "saves.remove('.ipynb_checkpoints')\n",
    "compliation_csv = pd.DataFrame(columns = ['save_name', 'accu_undefended', 'auc_undefended', 'accu_defended', 'auc_defended'])\n",
    "print(saves)\n",
    "att_comp = False\n",
    "\n",
    "output = defense_validation(model_defense, feature_extractor, features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_dir = '/home/jupyter/topk_datasets/datasets/{}'.format('e1e'), reference = TCGA_Lung_csv, val_list = val_test_file_list\n",
    "                               ,defense = 'std_filter', d = 3, attention_comp = att_comp, natural = True)\n",
    "accu_und, auc_und  = accu(output, answer_rows=['c1_undefended', 'c2_undefended'], reference = TCGA_Lung_csv), auc(output, answer_rows=['c1_undefended', 'c2_undefended'], reference = TCGA_Lung_csv) \n",
    "accu_def, auc_def = accu(output, answer_rows=['c1_defended', 'c2_defended'], reference = TCGA_Lung_csv), auc(output, answer_rows=['c1_defended', 'c2_defended'], reference = TCGA_Lung_csv)\n",
    "compliation_csv = compliation_csv.append(pd.DataFrame([['natural', accu_und, auc_und, accu_def, auc_def]], columns = compliation_csv.columns))\n",
    "compliation_csv.to_csv('/home/jupyter/defensive_complication_noac3.csv')\n",
    "\n",
    "for save in saves:\n",
    "    if 'h' in save and not att_comp:\n",
    "        \n",
    "        continue\n",
    "    current_adv_dataset_dir = os.path.join(saves_dir, save)\n",
    "\n",
    "    output = defense_validation(model_defense, feature_extractor, features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_dir = '/home/jupyter/topk_datasets/datasets/{}'.format(save), reference = TCGA_Lung_csv, val_list = val_test_file_list\n",
    "                               ,defense = 'std_filter', d = 3, attention_comp = att_comp)\n",
    "    accu_und, auc_und  = accu(output, answer_rows=['c1_undefended', 'c2_undefended'], reference = TCGA_Lung_csv), auc(output, answer_rows=['c1_undefended', 'c2_undefended'], reference = TCGA_Lung_csv) \n",
    "    accu_def, auc_def = accu(output, answer_rows=['c1_defended', 'c2_defended'], reference = TCGA_Lung_csv), auc(output, answer_rows=['c1_defended', 'c2_defended'], reference = TCGA_Lung_csv)\n",
    "    compliation_csv = compliation_csv.append(pd.DataFrame([[save, accu_und, auc_und, accu_def, auc_def]], columns = compliation_csv.columns))\n",
    "    compliation_csv.to_csv('/home/jupyter/defensive_complication_noac3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff2722-d370-4cdc-a4e8-733dcc7c96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accu(output, TCGA_Lung_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972ce9f-f0a9-4ea1-9f4d-5edbb3316054",
   "metadata": {
    "tags": []
   },
   "source": [
    "# framework demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72a0a0-fe25-400f-bbdd-b4e12e5af300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_adv_framework_analytics(model, feature_extractor, reference, slide_name, features_dir, slides_dir, topk = 10):\n",
    "    \n",
    "    full_feature_path = os.path.join(features_dir, '{}.h5'.format(slide_name))\n",
    "    with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "        features = hdf5_file['features'][:]\n",
    "        coords = hdf5_file['coords'][:]\n",
    "        features = torch.from_numpy(features).cuda()\n",
    "        slide_full_path = os.path.join(slides_dir,\n",
    "                                       '{}.svs'.format(slide_name))\n",
    "        wsi = openslide.open_slide(slide_full_path)\n",
    "\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        reference_label = reference_row['label'].item()\n",
    "\n",
    "        slide_resolution = int(reference_row['resolution'])\n",
    "        \n",
    "        if reference_label == 'LUAD':\n",
    "            attack_target = 0\n",
    "        else:\n",
    "            attack_target = 1\n",
    "        \n",
    "        if slide_resolution == 40:\n",
    "            patch_size = 512\n",
    "            downsample = 2\n",
    "        else:\n",
    "            patch_size = 256\n",
    "            downsample = 1\n",
    "    \n",
    "    y_collection, a_collection, idx_coord_list, img_collection, features_collection, noise_collection = instance_topk_attack(model, \n",
    "        feature_extractor, features, coords, wsi,\n",
    "        patch_size = patch_size, patch_level = 0, downsample = downsample, target = attack_target, topk = topk, e = 0.01,\n",
    "        f = 0.01, attention_comp = False, verbose = False)\n",
    "    \n",
    "    attack_features = features_collection[0]\n",
    "    \n",
    "    feature_diff_list = []\n",
    "    for i in range(topk):\n",
    "        idx, coord = idx_coord_list[i]\n",
    "        original_feature = features[idx, :]\n",
    "        curr_diff = original_feature - attack_features[i, :]\n",
    "        feature_diff_list += curr_diff.tolist()\n",
    "    \n",
    "        \n",
    "    diff_array = np.asarray(feature_diff_list)\n",
    "    plt.hist(diff_array, bins = 250)\n",
    "    plt.title('Distribution of changes in resnet extracted features natural/topk attack e = 0.01')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ad7cf-cd45-45ba-8063-7388fff94e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = '/home/ext_yao_gary_mayo_edu/lung-features/'\n",
    "slides_dir = '/home/ext_yao_gary_mayo_edu/lung-slides'\n",
    "target_slide = 'TCGA-49-6744-01Z-00-DX2.1982e585-65a4-4330-9140-ccabcdd106f8'\n",
    "moded_center = instance_adv_framework_analytics(model_defense, feature_extractor, TCGA_Lung_csv, target_slide, features_dir, slides_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba018c0-24c0-472c-a883-95af4c6c0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adv_instance(model, reference, slide_name, adv_dataset_dir, features_original, attention_comp = False):\n",
    "        if not attention_comp:\n",
    "            adv_feature_path = os.path.join(adv_dataset_dir, 'features', '{}_adv.tr'.format(slide_name))\n",
    "            adv_coords_path = os.path.join(adv_dataset_dir, 'coords', '{}_adv.p'.format(slide_name))\n",
    "            adv_img_path = os.path.join(adv_dataset_dir, 'img', '{}_adv.tr'.format(slide_name))\n",
    "        else:\n",
    "            adv_feature_path = os.path.join(adv_dataset_dir, 'features', '{}_att.tr'.format(slide_name))\n",
    "            adv_coords_path = os.path.join(adv_dataset_dir, 'coords', '{}_adv.p'.format(slide_name))\n",
    "            adv_img_path = os.path.join(adv_dataset_dir, 'img', '{}_att.tr'.format(slide_name))\n",
    "        idx_coords = None\n",
    "        try:\n",
    "            idx_coords = pickle.load(open(adv_coords_path, 'rb'))\n",
    "            adv_features = torch.load(adv_feature_path)\n",
    "            adv_img = torch.load(adv_img_path)\n",
    "        except:\n",
    "            print('Failed on {}'.format(slide_name))\n",
    "        if idx_coords == None:\n",
    "            return None\n",
    "        #hacki implemeantation\n",
    "        k = len(idx_coords)\n",
    "        #print(k)\n",
    "        with torch.no_grad():\n",
    "            l, y, y1, a, r = model(features_original)\n",
    "\n",
    "            reference_row = reference[reference['slide_name'] == slide_name]\n",
    "            reference_label = reference_row['label'].item()\n",
    "            if reference_label == 'LUAD':\n",
    "                target = 0\n",
    "            else:\n",
    "                target = 1\n",
    "\n",
    "            res, ind = a[target,0:].topk(k)\n",
    "\n",
    "            for i in range(k):\n",
    "                #print(features_original[ind[i]])\n",
    "                #print(adv_features[i])\n",
    "                features_original[ind[i], :] = adv_features[i]\n",
    "                \n",
    "        return features_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2a4a2-eb6a-4cd4-a5b7-f2f2e2b43958",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.001\n",
    "shuffle = False\n",
    "epoch_limit = 10\n",
    "train_ratio = .8\n",
    "att_comp = False\n",
    "adv_threshhold = 1052\n",
    "def train_epoch_p2(model, mock_model, reference, optimizer, train_list, feature_path, adv_dataset_path, attention_comp = False):\n",
    "    loss_sum = 0\n",
    "    results = pd.DataFrame(columns = ['slide_name', 'c1', 'c2'])\n",
    "    loss_metric = torch.nn.CrossEntropyLoss()\n",
    "    for slide_name in tqdm(train_list):\n",
    "        adverarial_ins = False\n",
    "        if '_adv' in slide_name:\n",
    "            slide_name = slide_name[:-4]\n",
    "            adverarial_ins = True\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        try:\n",
    "            label_eng = reference_row['label']\n",
    "            if label_eng.item() == 'LUAD':\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            label = torch.Tensor([label])\n",
    "            label = label.long().cuda()\n",
    "            full_feature_path = os.path.join(feature_path, '{}.h5'.format(slide_name))\n",
    "\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "        except:\n",
    "            print('Failed on {}'.format(slide_name))\n",
    "            continue\n",
    "        \n",
    "        features_original = torch.from_numpy(features).cuda()\n",
    "        \n",
    "        \n",
    "        #if current_index is bigger than natural dataset length, that means we are doing an adversarial training instance\n",
    "        if adverarial_ins:\n",
    "            features_original = load_adv_instance(mock_model, TCGA_Lung_csv, slide_name, adv_dataset_path, features_original, attention_comp = attention_comp)\n",
    "            slide_name = slide_name + '_adv'\n",
    "            \n",
    "        if features_original == None:\n",
    "            continue\n",
    "        model.zero_grad()\n",
    "        l, y, y1, a, r = model(features_original)\n",
    "        loss = loss_metric(y, label)\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        results = results.append(pd.DataFrame([[slide_name, y[0,0].item(), y[0,1].item()]], columns = results.columns))\n",
    "    return results, loss_sum\n",
    "        \n",
    "    \n",
    "def validate_epoch_p2(model, mock_model, reference, validate_list, feature_path, adv_dataset_path, adv = False, attention_comp = False):\n",
    "    results = pd.DataFrame(columns = ['slide_name', 'c1', 'c2'])\n",
    "    for slide_name in tqdm(validate_list):\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        label_eng = reference_row['label']\n",
    "        try:\n",
    "            if label_eng.item() == 'LUAD':\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            full_feature_path = os.path.join(feature_path, '{}.h5'.format(slide_name))\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "        except:\n",
    "            #print('Failed on {}'.format(slide_name))\n",
    "            continue\n",
    "            \n",
    "        features_original = torch.from_numpy(features).cuda()\n",
    "        if adv:\n",
    "            features_original = load_adv_instance(mock_model, TCGA_Lung_csv, slide_name, adv_dataset_path, features_original, attention_comp=attention_comp)\n",
    "        \n",
    "        if features_original == None:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            l, y, y1, a, r = model(features_original)\n",
    "        results = results.append(pd.DataFrame([[slide_name, y[0,0].item(), y[0,1].item()]], columns = results.columns))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d6761-92bd-459c-b3f4-18e2b6ea10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLAM_MB(n_classes=2, dropout=False)\n",
    "\n",
    "ckpt_path = '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt_clean = {}\n",
    "for key in ckpt.keys():\n",
    "    if 'instance_loss_fn' in key:\n",
    "        continue\n",
    "    key_new = key.replace('attention_net.module.3.attention', 'attention_net.module.2.attention')\n",
    "    ckpt_clean.update({key_new.replace('.module', ''):ckpt[key]})\n",
    "    \n",
    "model.load_state_dict(ckpt_clean, strict=True)\n",
    "model.cuda()\n",
    "mock_model = CLAM_MB(n_classes=2, dropout=False)\n",
    "\n",
    "ckpt_path = '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt_clean = {}\n",
    "for key in ckpt.keys():\n",
    "    if 'instance_loss_fn' in key:\n",
    "        continue\n",
    "    key_new = key.replace('attention_net.module.3.attention', 'attention_net.module.2.attention')\n",
    "    ckpt_clean.update({key_new.replace('.module', ''):ckpt[key]})\n",
    "    \n",
    "mock_model.load_state_dict(ckpt_clean, strict=True)\n",
    "mock_model.cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD([\n",
    "                {'params': model.parameters()},\n",
    "            ], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e946d8-11c3-4b40-afaa-1681df57026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_listed_split(num_samples = 1052, train_ratio = .8, adv = True):\n",
    "    num_train_samples = int(train_ratio * num_samples)\n",
    "    num_val_samples = num_samples - num_train_samples\n",
    "    base_range = np.arange(0, num_samples)\n",
    "    np.random.shuffle(base_range)\n",
    "    train_range = base_range[0:num_train_samples]\n",
    "    validate_range = base_range[num_train_samples:]\n",
    "    if adv:\n",
    "        train_range = np.append(train_range, train_range +num_samples)\n",
    "        \n",
    "    return train_range, validate_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ebac2-bc6d-492b-8318-31f73f43118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_file_list_with_adv = []\n",
    "for file in train_file_list:\n",
    "    train_file_list_with_adv.append(file)\n",
    "    train_file_list_with_adv.append(file + '_adv')\n",
    "random.shuffle(train_file_list_with_adv)\n",
    "random.shuffle(val_test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa23900-2dbb-469e-ab52-eca130a6d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounting_pd = pd.DataFrame(columns = ['epoch', 'loss', 'train_accu', 'accu', 'adv_accu'])\n",
    "for epoch in range(10):\n",
    "    results = validate_epoch_p2(model, mock_model, TCGA_Lung_csv, val_test_file_list, feature_path = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_path='/home/jupyter/topk_datasets/datasets/e1e_whole_final/', attention_comp = True)\n",
    "    accu_nat, auc_nat = accu(results, TCGA_Lung_csv), auc(results, TCGA_Lung_csv)\n",
    "    results = validate_epoch_p2(model, mock_model, TCGA_Lung_csv, val_test_file_list, feature_path = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_path='/home/jupyter/topk_datasets/datasets/e1e_whole_final/', adv = True, attention_comp = True)\n",
    "    accu_adv, auc_adv = accu(results, TCGA_Lung_csv), auc(results, TCGA_Lung_csv)\n",
    "    results, loss_sum = train_epoch_p2(model, mock_model, TCGA_Lung_csv, optimizer, train_file_list_with_adv, feature_path = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_path='/home/jupyter/topk_datasets/datasets/e1e_whole_final/', attention_comp=True)\n",
    "    accounting_pd = accounting_pd.append(pd.DataFrame([[epoch, loss_sum, 0, accu_nat, accu_adv]], columns = accounting_pd.columns))\n",
    "    accounting_pd.to_csv('/home/jupyter/adv_training_ac.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d32312-093a-4560-9d43-f31253b572e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_pipeline_adv_pass(model, feature_extractor, reference, slide_name, adv_dataset_dir, features_original, attention_comp = False):\n",
    "    if not attention_comp:\n",
    "        adv_feature_path = os.path.join(adv_dataset_dir, 'features', '{}_adv.tr'.format(slide_name))\n",
    "        adv_coords_path = os.path.join(adv_dataset_dir, 'coords', '{}_adv.p'.format(slide_name))\n",
    "        adv_img_path = os.path.join(adv_dataset_dir, 'img', '{}_adv.tr'.format(slide_name))\n",
    "    else:\n",
    "        adv_feature_path = os.path.join(adv_dataset_dir, 'features', '{}_att.tr'.format(slide_name))\n",
    "        adv_coords_path = os.path.join(adv_dataset_dir, 'coords', '{}_adv.p'.format(slide_name))\n",
    "        adv_img_path = os.path.join(adv_dataset_dir, 'img', '{}_att.tr'.format(slide_name))\n",
    "        idx_coords = None\n",
    "    idx_coords = pickle.load(open(adv_coords_path, 'rb'))\n",
    "    adv_features = torch.load(adv_feature_path)\n",
    "    adv_img = torch.load(adv_img_path)\n",
    "    if idx_coords == None:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    adv_features = feature_extractor(adv_img)\n",
    "    k = len(idx_coords)\n",
    "    for (i, (row, loc)) in enumerate(idx_coords):\n",
    "        features_original[row, :] = adv_features[i,:]\n",
    "        \n",
    "    l, y, y1, a ,r = model(features_original)\n",
    "    return l, y, y1, a, r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e60b9-111e-4a04-a56b-83bbd3739b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.001\n",
    "shuffle = False\n",
    "epoch_limit = 10\n",
    "train_ratio = .8\n",
    "adv_threshhold = 1052\n",
    "def train_epoch(model, feature_extractor, reference, optimizer, train_list, feature_path, adv_dataset_path, attention_comp = False):\n",
    "    loss_sum = 0\n",
    "    results = pd.DataFrame(columns = ['slide_name', 'c1', 'c2'])\n",
    "    loss_metric = torch.nn.CrossEntropyLoss()\n",
    "    for slide_name in tqdm(train_list):\n",
    "        adverarial_ins = False\n",
    "        if '_adv' in slide_name:\n",
    "            slide_name = slide_name[:-4]\n",
    "            adverarial_ins = True\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        try:\n",
    "            label_eng = reference_row['label']\n",
    "            if label_eng.item() == 'LUAD':\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            label = torch.Tensor([label])\n",
    "            label = label.long().cuda()\n",
    "            full_feature_path = os.path.join(feature_path, '{}.h5'.format(slide_name))\n",
    "\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "            if adverarial_ins:\n",
    "                adv_coords_path = os.path.join(adv_dataset_path, 'coords', '{}_adv.p'.format(slide_name))\n",
    "                idx_coords = pickle.load(open(adv_coords_path, 'rb'))\n",
    "        except:\n",
    "            print('Failed on {}'.format(slide_name))\n",
    "            continue\n",
    "        \n",
    "        features_original = torch.from_numpy(features).cuda()\n",
    "        \n",
    "        \n",
    "        #if current_index is bigger than natural dataset length, that means we are doing an adversarial training instance\n",
    "            \n",
    "        if features_original == None:\n",
    "            continue\n",
    "        model.zero_grad()\n",
    "        feature_extractor.zero_grad()\n",
    "        if adverarial_ins:\n",
    "            l, y, y1, a, r = whole_pipeline_adv_pass(model, feature_extractor, TCGA_Lung_csv, slide_name, adv_dataset_path, features_original, attention_comp=attention_comp)  \n",
    "        else:\n",
    "            l, y, y1, a, r = model(features_original)\n",
    "        \n",
    "        loss = loss_metric(y, label)\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        results = results.append(pd.DataFrame([[slide_name, y[0,0].item(), y[0,1].item()]], columns = results.columns))\n",
    "    return results, loss_sum\n",
    "        \n",
    "    \n",
    "def validate_epoch(model, feature_extractor, reference, validate_list, feature_path, adv_dataset_path, adv = False, attention_comp = False):\n",
    "    results = pd.DataFrame(columns = ['slide_name', 'c1', 'c2'])\n",
    "    for slide_name in tqdm(validate_list):\n",
    "        reference_row = reference[reference['slide_name'] == slide_name]\n",
    "        label_eng = reference_row['label']\n",
    "        try:\n",
    "            if label_eng.item() == 'LUAD':\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            full_feature_path = os.path.join(feature_path, '{}.h5'.format(slide_name))\n",
    "            with h5py.File(full_feature_path, 'r') as hdf5_file:\n",
    "                features = hdf5_file['features'][:]\n",
    "                coords = hdf5_file['coords'][:]\n",
    "        except:\n",
    "            #print('Failed on {}'.format(slide_name))\n",
    "            continue\n",
    "            \n",
    "        features_original = torch.from_numpy(features).cuda()\n",
    "        with torch.no_grad():\n",
    "            if adv:\n",
    "                l, y, y1, a, r = whole_pipeline_adv_pass(model, feature_extractor, TCGA_Lung_csv, slide_name, adv_dataset_path, features_original, attention_comp=attention_comp)  \n",
    "            else:\n",
    "                l, y, y1, a, r = model(features_original)\n",
    "        results = results.append(pd.DataFrame([[slide_name, y[0,0].item(), y[0,1].item()]], columns = results.columns))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cc756-d437-4cd3-907a-390299ef5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLAM_MB(n_classes=2, dropout=False)\n",
    "\n",
    "ckpt_path = '/home/ext_yao_gary_mayo_edu/CLAM_saves/100s_2_checkpoint.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "ckpt_clean = {}\n",
    "for key in ckpt.keys():\n",
    "    if 'instance_loss_fn' in key:\n",
    "        continue\n",
    "    key_new = key.replace('attention_net.module.3.attention', 'attention_net.module.2.attention')\n",
    "    ckpt_clean.update({key_new.replace('.module', ''):ckpt[key]})\n",
    "    \n",
    "model.load_state_dict(ckpt_clean, strict=True)\n",
    "model.cuda()\n",
    "\n",
    "feature_extractor = resnet50_baseline(pretrained=True)\n",
    "feature_extractor.eval()\n",
    "feature_extractor.cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "all_parameters = list(model.parameters()) + list(feature_extractor.parameters())\n",
    "optimizer = optim.SGD([\n",
    "                {'params': all_parameters},\n",
    "            ], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974dc4d-7ece-4b19-8585-7c770005bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounting_pd = pd.DataFrame(columns = ['epoch', 'loss', 'train_accu', 'accu', 'adv_accu'])\n",
    "attention_comp = False\n",
    "for epoch in range(10):\n",
    "    results = validate_epoch(model, feature_extractor, TCGA_Lung_csv, val_test_file_list, feature_path = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_path='/home/jupyter/topk_datasets/datasets/e1e_whole_final/', attention_comp = attention_comp)\n",
    "    accu_nat, auc_nat = accu(results, TCGA_Lung_csv), auc(results, TCGA_Lung_csv)\n",
    "    results = validate_epoch(model, feature_extractor, TCGA_Lung_csv, val_test_file_list, feature_path = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_path='/home/jupyter/topk_datasets/datasets/e1e_whole_final/', adv = True, attention_comp = attention_comp)\n",
    "    accu_adv, auc_adv = accu(results, TCGA_Lung_csv), auc(results, TCGA_Lung_csv)\n",
    "    results, loss_sum = train_epoch(model, feature_extractor, TCGA_Lung_csv, optimizer, train_file_list_with_adv, feature_path = '/home/ext_yao_gary_mayo_edu/lung-features/', adv_dataset_path='/home/jupyter/topk_datasets/datasets/e1e_whole_final/', attention_comp = attention_comp)\n",
    "    accounting_pd = accounting_pd.append(pd.DataFrame([[epoch, loss_sum, 0, accu_nat, accu_adv]], columns = accounting_pd.columns))\n",
    "    accounting_pd.to_csv('/home/jupyter/adv_whole_training_noacp2.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108b8fc-dfe1-4981-9a5d-674ce0c52a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_coord_path = '/home/jupyter/topk_datasets/datasets/e1e_whole_final/coords/TCGA-XC-AA0X-01Z-00-DX1.61A34BE0-F16B-4EC1-8E7F-7BF94F6629F4_adv.p' \n",
    "example_coord = pickle.load(open(example_coord_path, 'rb'))\n",
    "print(example_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649381c-ef11-4633-8689-18fb4c7e3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
